{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. Install and Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mediapipe in c:\\users\\sarac\\anaconda3\\envs\\mediapipe\\lib\\site-packages (0.10.21)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\sarac\\anaconda3\\envs\\mediapipe\\lib\\site-packages (4.11.0.86)\n",
      "Requirement already satisfied: absl-py in c:\\users\\sarac\\anaconda3\\envs\\mediapipe\\lib\\site-packages (from mediapipe) (2.3.1)\n",
      "Requirement already satisfied: attrs>=19.1.0 in c:\\users\\sarac\\anaconda3\\envs\\mediapipe\\lib\\site-packages (from mediapipe) (24.3.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\sarac\\anaconda3\\envs\\mediapipe\\lib\\site-packages (from mediapipe) (25.2.10)\n",
      "Requirement already satisfied: jax in c:\\users\\sarac\\anaconda3\\envs\\mediapipe\\lib\\site-packages (from mediapipe) (0.4.30)\n",
      "Requirement already satisfied: jaxlib in c:\\users\\sarac\\anaconda3\\envs\\mediapipe\\lib\\site-packages (from mediapipe) (0.4.30)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\sarac\\anaconda3\\envs\\mediapipe\\lib\\site-packages (from mediapipe) (3.9.4)\n",
      "Requirement already satisfied: numpy<2 in c:\\users\\sarac\\anaconda3\\envs\\mediapipe\\lib\\site-packages (from mediapipe) (1.26.4)\n",
      "Requirement already satisfied: opencv-contrib-python in c:\\users\\sarac\\anaconda3\\envs\\mediapipe\\lib\\site-packages (from mediapipe) (4.11.0.86)\n",
      "Requirement already satisfied: protobuf<5,>=4.25.3 in c:\\users\\sarac\\anaconda3\\envs\\mediapipe\\lib\\site-packages (from mediapipe) (4.25.8)\n",
      "Requirement already satisfied: sounddevice>=0.4.4 in c:\\users\\sarac\\anaconda3\\envs\\mediapipe\\lib\\site-packages (from mediapipe) (0.5.2)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\sarac\\anaconda3\\envs\\mediapipe\\lib\\site-packages (from mediapipe) (0.2.1)\n",
      "Requirement already satisfied: CFFI>=1.0 in c:\\users\\sarac\\anaconda3\\envs\\mediapipe\\lib\\site-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\sarac\\anaconda3\\envs\\mediapipe\\lib\\site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.23)\n",
      "Requirement already satisfied: ml-dtypes>=0.2.0 in c:\\users\\sarac\\anaconda3\\envs\\mediapipe\\lib\\site-packages (from jax->mediapipe) (0.5.3)\n",
      "Requirement already satisfied: opt-einsum in c:\\users\\sarac\\anaconda3\\envs\\mediapipe\\lib\\site-packages (from jax->mediapipe) (3.4.0)\n",
      "Requirement already satisfied: scipy>=1.9 in c:\\users\\sarac\\anaconda3\\envs\\mediapipe\\lib\\site-packages (from jax->mediapipe) (1.13.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.6 in c:\\users\\sarac\\anaconda3\\envs\\mediapipe\\lib\\site-packages (from jax->mediapipe) (8.5.0)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\sarac\\anaconda3\\envs\\mediapipe\\lib\\site-packages (from importlib-metadata>=4.6->jax->mediapipe) (3.21.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\sarac\\anaconda3\\envs\\mediapipe\\lib\\site-packages (from matplotlib->mediapipe) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\sarac\\anaconda3\\envs\\mediapipe\\lib\\site-packages (from matplotlib->mediapipe) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\sarac\\anaconda3\\envs\\mediapipe\\lib\\site-packages (from matplotlib->mediapipe) (4.59.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\sarac\\anaconda3\\envs\\mediapipe\\lib\\site-packages (from matplotlib->mediapipe) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\sarac\\anaconda3\\envs\\mediapipe\\lib\\site-packages (from matplotlib->mediapipe) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\sarac\\anaconda3\\envs\\mediapipe\\lib\\site-packages (from matplotlib->mediapipe) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\sarac\\anaconda3\\envs\\mediapipe\\lib\\site-packages (from matplotlib->mediapipe) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\sarac\\anaconda3\\envs\\mediapipe\\lib\\site-packages (from matplotlib->mediapipe) (2.9.0.post0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\users\\sarac\\anaconda3\\envs\\mediapipe\\lib\\site-packages (from matplotlib->mediapipe) (6.5.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sarac\\anaconda3\\envs\\mediapipe\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install mediapipe opencv-python\n",
    "import mediapipe as mp\n",
    "import cv2\n",
    "import time\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpDraw = mp.solutions.drawing_utils #Call the drawing tool\n",
    "mpHands = mp.solutions.hands #Call the hand tracking tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two-Hand Tracking\n",
    "\n",
    " <img src=https://i.imgur.com/qpRACer.png />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Setup up the hand tracking module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hands = mpHands.Hands(  \n",
    "    static_image_mode=False, #Image or Video (True: still image, False: steam mode)\n",
    "    #model_complexity=0,#0->compact model(fast speed)，1->full mode(slow speed)\n",
    "    max_num_hands=2, #How many hands are allowed to be recognized\n",
    "    min_detection_confidence=0.5, #confidence for hand detection\n",
    "    min_tracking_confidence=0.5 #confidence for hand tracking\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "while cap.isOpened(): #Is camera open or not\n",
    "    stime=time.time()\n",
    "    ret, frame = cap.read() #read camera data\n",
    "    h, w, c = frame.shape #get resolution(width/height) of the camera\n",
    "    frame=cv2.flip(frame,1) #flip image：-1:up and down、0: up and down, left and right、1:  left and right\n",
    "    imgRGB = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) #convert color channel from BGR(opencv) to RGB(mediapipe)\n",
    "    results = hands.process(imgRGB) #tracking hands and get the results\n",
    "       \n",
    "    if results.multi_hand_landmarks: #If find any hand\n",
    "        for i in range(len(results.multi_handedness)): #get the numbers of detected hands\n",
    "            thisHandType=results.multi_handedness[i].classification[0].label #get properties of the detected hand      \n",
    "            thisHand=results.multi_hand_landmarks[i] #get hand label information\n",
    "            mpDraw.draw_landmarks(frame, thisHand, mpHands.HAND_CONNECTIONS) #draw tools\n",
    "            #draw hands in the screen\n",
    "            for id, lm in enumerate(thisHand.landmark): #id=number,lm=coordinate               \n",
    "                hx, hy = int(lm.x * w), int(lm.y * h) #get coordinate of the joint\n",
    "                cv2.circle(frame, (hx, hy), 5, (255, 0, 0), cv2.FILLED) #make circles with blue color\n",
    "                cv2.putText(frame,str(id),(hx,hy), cv2.FONT_HERSHEY_PLAIN, 1, (255, 0, 255), 1)\n",
    "                if id==0:\n",
    "                    cv2.putText(frame,thisHandType,(hx,hy-30), cv2.FONT_HERSHEY_PLAIN, 2, (0, 255, 0), 2)            \n",
    "    etime=time.time()\n",
    "    fps=round(1/(etime-stime),2)\n",
    "    cv2.putText(frame,\"FPS:\" + str(fps),(10,50), cv2.FONT_HERSHEY_PLAIN, 3, (0, 0, 255), 3)\n",
    "    cv2.imshow('Webcam',frame) #display results on the screen\n",
    "    key=cv2.waitKey(1) #waitting for user's inputs\n",
    "    if key==ord('a'):  # 'a': capture photo\n",
    "        cv2.imwrite('webcam.jpg',frame) # save file\n",
    "    if key==ord('q'):  #'q': quit\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hand Pose Recognition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup the Hand tracking module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mediapipe opencv-python\n",
    "import mediapipe as mp\n",
    "import cv2\n",
    "import time\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpDraw = mp.solutions.drawing_utils #Call the drawing tool\n",
    "mpHands = mp.solutions.hands #Call the hand tracking tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hands = mpHands.Hands(  \n",
    "    static_image_mode=False, #Image or Video (True: still image, False: stream mode)\n",
    "    model_complexity=0,#0->compact model(fast speed)，1->full mode(slow speed)\n",
    "    max_num_hands=2,  #How many hands are allowed to be recognized\n",
    "    min_detection_confidence=0.7, #confidence for hand detection\n",
    "    min_tracking_confidence=0.5 #confidence for hand tracking\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AngleTH=130 #threshold(TH) angle: the joint in range of motion\n",
    "def findAngleF(a,b,c):    \n",
    "    ang = math.degrees(math.atan2(c[2]-b[2], c[1]-b[1]) - math.atan2(a[2]-b[2], a[1]-b[1]))\n",
    "    if ang<0 :\n",
    "      ang=ang+360\n",
    "    if ang >= 360- ang:\n",
    "        ang=360-ang\n",
    "    return round(ang,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "while cap.isOpened(): #Is camera open or not\n",
    "    stime=time.time()\n",
    "    ret, frame = cap.read() #read camera data\n",
    "    h, w, c = frame.shape  #get resolution(width/height) of the camera\n",
    "    frame=cv2.flip(frame,1) #flip image：-1:up and down、0: up and down, left and right、1:  left and right\n",
    "    imgRGB = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) #convert color channel from BGR(opencv) to RGB(mediapipe)\n",
    "    results = hands.process(imgRGB) #tracking hands and get the results\n",
    "       \n",
    "    if results.multi_hand_landmarks: #If there is any hand available\n",
    "        for i in range(len(results.multi_handedness)): #get the numbers of detected hands\n",
    "            thisHandType=results.multi_handedness[i].classification[0].label #get properties of the detected hand          \n",
    "            thisHand=results.multi_hand_landmarks[i] #get hand label information \n",
    "            mpDraw.draw_landmarks(frame, thisHand, mpHands.HAND_CONNECTIONS) #draw tools\n",
    "            thisHandLMList = []\n",
    "            for id, lm in enumerate(thisHand.landmark): #id=number,lm=coordinate                  \n",
    "                thisHandLMList.append([id, lm.x, lm.y,lm.z])\n",
    "                hx, hy = int(lm.x * w), int(lm.y * h) #get coordinate of the joint\n",
    "                cv2.circle(frame, (hx, hy), 5, (255, 0, 0), cv2.FILLED)  #make circles with blue color\n",
    "                cv2.putText(frame,str(id),(hx,hy), cv2.FONT_HERSHEY_PLAIN, 1, (255, 0, 255), 1)\n",
    "                if id==0:\n",
    "                    cv2.putText(frame,thisHandType,(hx,hy-30), cv2.FONT_HERSHEY_PLAIN, 2, (0, 255, 0), 2)            \n",
    "            finger=[0,0,0,0,0]\n",
    "            if (findAngleF(thisHandLMList[0],thisHandLMList[3],thisHandLMList[4])>AngleTH):\n",
    "                finger[0]=1\n",
    "            if (findAngleF(thisHandLMList[0],thisHandLMList[6],thisHandLMList[8])>AngleTH):\n",
    "                finger[1]=1\n",
    "            if (findAngleF(thisHandLMList[0],thisHandLMList[10],thisHandLMList[12])>AngleTH):\n",
    "                finger[2]=1\n",
    "            if (findAngleF(thisHandLMList[0],thisHandLMList[14],thisHandLMList[16])>AngleTH):\n",
    "                finger[3]=1\n",
    "            if (findAngleF(thisHandLMList[0],thisHandLMList[18],thisHandLMList[20])>AngleTH):\n",
    "                finger[4]=1\n",
    "            #print(finger)\n",
    "\n",
    "            #-----------------Recognizing hand gestures------------------------\n",
    "            text=\"\"#     Thumb,Index,Middle,Ring,Little finger\n",
    "            if (finger==[1,0,0,0,0]):\n",
    "                text=\"Good\"\n",
    "            if (finger==[0,0,1,1,1]):\n",
    "                text=\"OK\"            \n",
    "            if (finger==[1,1,1,1,1]):\n",
    "                text=\"paper\"            \n",
    "            if (finger==[0,1,1,0,0]):\n",
    "                text=\"scissors\"\n",
    "            if (finger==[1,1,0,0,1]):\n",
    "                text=\"Spider-Man\"\n",
    "            if (finger==[0,0,0,0,0]):\n",
    "                text=\"rock\"\n",
    "                \n",
    "            \n",
    "            #           Image   text   coordinate       font style   font size   font color  thickness\n",
    "            \n",
    "            cv2.putText(frame, text, (0, 200), cv2.FONT_HERSHEY_PLAIN, 5  , (255, 0, 0), 5) #put text to the screen\n",
    "        \n",
    "            \n",
    "\n",
    "    etime=time.time()\n",
    "    fps=round(1/(etime-stime),2)\n",
    "    cv2.putText(frame,\"FPS:\" + str(fps),(10,50), cv2.FONT_HERSHEY_PLAIN, 3, (0, 0, 255), 3)\n",
    "\n",
    "    cv2.imshow('Webcam',frame) #display results on the screen\n",
    "    key=cv2.waitKey(1) #waitting for user's inputs\n",
    "    if key==ord('a'):   # 'a': capture photo\n",
    "        cv2.imwrite('webcam.jpg',frame) # save file\n",
    "    if key==ord('q'):  #'q': quit\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Mushroom Catching Game Instructions\n",
    "\n",
    "#Please prepare an audio file 'eat.mp3' for the sound effect when catching is successful.\n",
    "\n",
    "#Please prepare two image files 'b.jpg' and 'm.jpg'.\n",
    "\n",
    "#The image processing in the program will remove the white background and resize the images, so please use images with a white background or in PNG format.\n",
    "\n",
    "#The background removal settings can be found on line 59, inside the 'cv2.threshold' function. Pixels with values greater than 'thresh' will be deleted.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time\n",
    "import mediapipe as mp\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import playsound #pip3 install playsound==1.2.2 #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpDraw = mp.solutions.drawing_utils #Call the drawing tool\n",
    "mpHands = mp.solutions.hands #Call the hand tracking tool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call the hand recognition tool within the hand tracking tool\n",
    "hands = mpHands.Hands(  \n",
    "    static_image_mode=False,  #Image or Video (True: still image, False: stream mode)\n",
    "    #model_complexity=0,#0->compact model(fast speed)，1->full mode(slow speed)\n",
    "    max_num_hands=1, #How many hands are allowed to be recognized\n",
    "    min_detection_confidence=0.5, #confidence for hand detection\n",
    "    min_tracking_confidence=0.5  #confidence for hand tracking\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load object image (white background can be removed)\n",
    "ObjSizeX,ObjSizeY=80,80\n",
    "BucketSizeX,BucketSizeY=180,80\n",
    "Obj = cv2.resize(cv2.imread('l.png'),(ObjSizeX,ObjSizeY)) #Object image\n",
    "#Load basket image\n",
    "Bucket = cv2.resize(cv2.imread('b.png'),(BucketSizeX,BucketSizeY)) #Object image\n",
    "#Grabbing Status\n",
    "catchObj=False\n",
    "#Whether to generate a New Object\n",
    "NewObj=True\n",
    "#Obj Starting Position\n",
    "x,y=0,0\n",
    "#Score\n",
    "Score=0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AngleTH=130 #Determine Opening Angle\n",
    "def findAngleF(a,b,c):    \n",
    "    ang = math.degrees(math.atan2(c[2]-b[2], c[1]-b[1]) - math.atan2(a[2]-b[2], a[1]-b[1]))\n",
    "    if ang<0 :\n",
    "      ang=ang+360\n",
    "    if ang >= 360- ang:\n",
    "        ang=360-ang\n",
    "    return round(ang,2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drawing: Placing the object at a specific position on the background, removing the white border\n",
    "def addPng(ax,ay,bg,png):\n",
    "    global NewObj\n",
    "    pngY,pngX,channels = png.shape        \n",
    "    bgY,bgX,channels = bg.shape\n",
    "    if (ax+pngX<=bgX and ax>=0) and (ay+pngY<=bgY and ay>=0):\n",
    "        roi = bg[ay:ay+pngY, ax:ax+pngX ] #Placement Position\n",
    "        #1. Create a black PNG portion as a mask\n",
    "        pnggray = cv2.cvtColor(png,cv2.COLOR_BGR2GRAY)\n",
    "        ret, mask = cv2.threshold(pnggray, thresh=230, maxval=255, type=cv2.THRESH_BINARY) #Less than thresh becomes 0, the rest become maxval.\n",
    "        mask_inv = cv2.bitwise_not(mask) #Inverse\n",
    "        #cv2.imshow('mask',mask_inv)\n",
    "        #cv2.waitKey(0)\n",
    "        #2.get the background area\n",
    "        bg_roi = cv2.bitwise_and(roi,roi,mask = mask) # Extract the remaining background\n",
    "        #cv2.imshow('mask',bg_roi)\n",
    "        #cv2.waitKey(0)\n",
    "        #3.get the Logo area\n",
    "        png_roi = cv2.bitwise_and(png,png,mask = mask_inv) #Retrieve the actual display area\n",
    "        #cv2.imshow('mask',png_roi)\n",
    "        #cv2.waitKey(0)\n",
    "        #4.making the final presenation\n",
    "        dst = cv2.add(bg_roi,png_roi)\n",
    "        bg[ay:ay+pngY, ax:ax+pngX ] = dst\n",
    "        return bg\n",
    "    else:\n",
    "        NewObj=True\n",
    "        return bg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Detecting Palm Status\n",
    "def fist(img):\n",
    "    imgRGB = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) #convert color channel from BGR(opencv) to RGB(mediapipe)\n",
    "    h, w, c = frame.shape #get resolution(width/height) of the camera\n",
    "    results = hands.process(imgRGB)  #tracking hands and get the results\n",
    "\n",
    "    if results.multi_hand_landmarks: #If there is any hand available\n",
    "        for i in range(len(results.multi_handedness)): #get the numbers of detected hands\n",
    "            thisHandType=results.multi_handedness[i].classification[0].label #get properties of the detected hand            \n",
    "            thisHand=results.multi_hand_landmarks[i] #get hand label information\n",
    "            mpDraw.draw_landmarks(frame, thisHand, mpHands.HAND_CONNECTIONS) #draw tools\n",
    "            thisHandLMList = []\n",
    "            for id, lm in enumerate(thisHand.landmark): #id=number,lm=coordinate                 \n",
    "                thisHandLMList.append([id, lm.x, lm.y,lm.z])\n",
    "                hx, hy = int(lm.x * w), int(lm.y * h)  #get coordinate of the joint\n",
    "                cv2.circle(frame, (hx, hy), 5, (255, 0, 0), cv2.FILLED) #make circles with blue color\n",
    "                cv2.putText(frame,str(id),(hx,hy), cv2.FONT_HERSHEY_PLAIN, 1, (255, 0, 255), 1)\n",
    "                if id==0:\n",
    "                    cv2.putText(frame,thisHandType,(hx,hy-30), cv2.FONT_HERSHEY_PLAIN, 2, (0, 255, 0), 2)            \n",
    "            finger=[0,0,0,0,0]\n",
    "            if (findAngleF(thisHandLMList[0],thisHandLMList[3],thisHandLMList[4])>AngleTH):\n",
    "                finger[0]=1\n",
    "            if (findAngleF(thisHandLMList[0],thisHandLMList[6],thisHandLMList[8])>AngleTH):\n",
    "                finger[1]=1\n",
    "            if (findAngleF(thisHandLMList[0],thisHandLMList[10],thisHandLMList[12])>AngleTH):\n",
    "                finger[2]=1\n",
    "            if (findAngleF(thisHandLMList[0],thisHandLMList[14],thisHandLMList[16])>AngleTH):\n",
    "                finger[3]=1\n",
    "            if (findAngleF(thisHandLMList[0],thisHandLMList[18],thisHandLMList[20])>AngleTH):\n",
    "                finger[4]=1\n",
    "            #print(finger)\n",
    "\n",
    "            totalFingers=finger.count(1)    \n",
    "            x1,y1=np.amin(np.array(thisHandLMList)[:,1]),np.amin(np.array(thisHandLMList)[:,2])\n",
    "            x2,y2=np.amax(np.array(thisHandLMList)[:,1]),np.amax(np.array(thisHandLMList)[:,2])  \n",
    "            return totalFingers,x1,y1,x2,y2\n",
    "    else:\n",
    "        return None,None,None,None,None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "while cap.isOpened():  #Is camera open or not\n",
    "    stime=time.time()\n",
    "    ret, frame = cap.read() #read camera data\n",
    "    h, w, c = frame.shape #get resolution(width/height) of the camera\n",
    "    frame=cv2.flip(frame,1) #flip image：-1:up and down、0: up and down, left and right、1:  left and right\n",
    "\n",
    "    #Detecting Grasp\n",
    "    totalFingers,hx1,hy1,hx2,hy2=fist(frame)\n",
    "    if not (totalFingers==None): #Hand Detected\n",
    "        if totalFingers<=1: #Fist Clenched\n",
    "            #Get Palm Area\n",
    "            #print(hx1,hy1,hx2,hy2)\n",
    "            if ((x+ObjSizeX//2)>=hx1*w and (x+ObjSizeX//2)<=hx2*w and (y+ObjSizeY//2)>=hy1*h and (y+ObjSizeY//2)<=hy2*h):#If the object is within the palm\n",
    "                #print(\"Catched\")\n",
    "                catchObj=True\n",
    "            else:\n",
    "                #print(\"No\")\n",
    "                catchObj=False\n",
    "        else:\n",
    "            catchObj=False\n",
    "    else:\n",
    "        catchObj=False\n",
    "\n",
    " #Basket is in the middle at the bottom\n",
    "    BucketX=round((w-BucketSizeX)/2)#middle\n",
    "    BucketY=round(h-BucketSizeY/2-50)#bottom\n",
    "    frame=addPng(BucketX,BucketY,frame,Bucket)    \n",
    " \n",
    "    if catchObj :#Object Grabbing Process\n",
    "        #Object follows Palm\n",
    "        x,y=round(((hx1+hx2)*w-ObjSizeX)//2),round(((hy1+hy2)*h-ObjSizeY)//2)\n",
    "        frame=addPng(x,y,frame,Obj)\n",
    "        #Determine if the object is inside the basket----------------------------------------\n",
    "        if ((x+ObjSizeX//2)>=BucketX and (x+ObjSizeX//2)<=BucketX+BucketSizeX and (y+ObjSizeY//2)>=BucketY and (y+ObjSizeY//2)<=BucketY+BucketSizeY):\n",
    "            Score=Score+1\n",
    "            print(\"Score=\" + str(Score))\n",
    "            NewObj=True\n",
    "            catchObj=False\n",
    "            playsound.playsound(\"eat.mp3\")\n",
    "\n",
    "\n",
    "            #Reset Coordinates\n",
    "            x,y=random.randint(10,w-ObjSizeX-10) ,5\n",
    "    elif(NewObj==False):#Freefall Process        \n",
    "        y=y+5\n",
    "        frame=addPng(x,y,frame,Obj)\n",
    "    else:#Generate New Object Process\n",
    "        x,y=random.randint(10,w-ObjSizeX-10) ,5\n",
    "        NewObj=False  \n",
    "    etime=time.time()\n",
    "    fps=round(1/(etime-stime),2)\n",
    "    cv2.putText(frame, \"Get \" + str(Score) , (10, 70), cv2.FONT_HERSHEY_PLAIN, 3, (255, 0, 255), 3)    \n",
    "    cv2.putText(frame, \"FPS \" + str(fps) , (w-300, 70), cv2.FONT_HERSHEY_PLAIN, 3, (0, 255, 255), 3)    \n",
    "    cv2.imshow('frame', frame)\n",
    "   \n",
    "    key=cv2.waitKey(1) #Waiting for User Keyboard Input\n",
    "    if key==ord('a'):  #'a': capture photo\n",
    "        cv2.imwrite('webcam.jpg',frame) #save file\n",
    "    if key==ord('q'):  #'q': quit\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
